{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: yellow; font-size: 24px; font-weight: bold;\">\n",
    "    DECISION TREES\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A decision tree is a Machine Learning model that gives the decision after considering certain conditions.\n",
    "- Looks like tree structure.\n",
    "- Both Regression and Classification tasks.\n",
    "- Decision trees Non parametric Algorithm(does not involve any equation)\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*kwCh2-U02xf-EWaTt3Xr4w.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/0*PB7MYQfzyaLaTp1n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iamge](https://miro.medium.com/v2/resize:fit:1400/0*fN_oh7NDPd3Y_qZr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://images.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-size: 24px\">DECISION TREE ALGORITHM</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ID3 - Iterative Dichotomiser 3 (updated version of c4.5)\n",
    "2. CART - Classification And Regression Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ID3 (Iterative Dichotomiser 3)\n",
    "\n",
    "- Developed by: Ross Quinlan in 1986.\n",
    "- Splitting Criterion: Uses Information Gain to select the attribute that provides the most information about the class label. It measures the - reduction in entropy (uncertainty) after a dataset is split on an attribute.\n",
    "- Handling of Data:\n",
    "Works only with categorical data. Continuous attributes must be discretized before use.\n",
    "Does not handle missing values effectively.\n",
    "- Tree Structure: Produces a multi-way tree, meaning each node can have multiple branches (one for each category of the attribute).\n",
    "- Pruning: Does not incorporate pruning; thus, it can overfit the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. C4.5\n",
    "\n",
    "- Developed by: Ross Quinlan, as an extension of ID3 in 1993.\n",
    "- Splitting Criterion: Uses Gain Ratio, which adjusts the information gain by taking into account the intrinsic information of a split. This helps to prevent a bias towards attributes with many categories.\n",
    "- Handling of Data:\n",
    "Can handle both categorical and continuous attributes natively.\n",
    "Can manage missing values by assigning a probability distribution to the missing data.\n",
    "- Tree Structure: Also produces a multi-way tree but includes the ability to handle continuous attributes without prior discretization.\n",
    "- Pruning: Incorporates post-pruning techniques to reduce overfitting, improving generalization on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. CART (Classification and Regression Trees)\n",
    "\n",
    "- Developed by: Breiman et al. in 1986.\n",
    "- Splitting Criterion: Uses Gini Impurity for classification tasks and Mean Squared Error for regression tasks. Gini impurity measures the impurity of a dataset and aims to minimize it.\n",
    "- Handling of Data:\n",
    "Can handle both categorical and continuous data.\n",
    "Missing values can be handled by surrogate splits, which allow the algorithm to effectively utilize available data.\n",
    "- Tree Structure: Produces binary trees, meaning each node splits into exactly two branches, which simplifies the structure.\n",
    "- Pruning: Uses cost complexity pruning (also called weakest link pruning) to avoid overfitting, balancing tree depth and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:purple;font-size: 24px\">SELECTION OF ROOT NODE</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Entropy\n",
    "2. Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow;font-size: 24px\">ENTROPY</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy represents the randomness of data.\n",
    "- When entropy is more the randomness will be more.That means,the data point are distributed here and there.\n",
    "- It also indicates impurities in the data which keep the data points apart.\n",
    "- When entropy is high,randomness is high and hence there are more impurities.In this case,the output may not be accurate.\n",
    "- When entropy is low,randomness is low and hence the data is close together without much impurities.Such data is useful for the decision tree to make correct decision.\n",
    "- INFORMATION GAIN(IG) :\n",
    "    * When entropy is low,the data cointains less impurites.When impurities are less provide more useful information to the decision tree algorithm.This is called information gain(IG).\n",
    "    * This is the reason entropy is generally applied on each node in the decision tree to calculate information gain.When the information gain is highest for a node,it should be taken as a <b>Root Node.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://i.ytimg.com/vi/ER2AahO9qGY/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*MshQ-gpXdjT_2YKqaihPTg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size: 24px\">GINI INDEX</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gini Index is a direct measurement of impurities in the data.\n",
    "- When gini index value is high,impurities are high.When it is low,impurities are low.Hence,we should consider that column having lowest gini index as root node.\n",
    "\n",
    "![image](https://i.sstatic.net/E7Fak.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iamge](https://favtutor.com/resources/images/uploads/mceu_50362918251623230269550.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://image.slidesharecdn.com/workingwithlogisticregressionv1-190103063213/85/Simple-Decision-Tree-6-320.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow;font-size: 24px\">COMPARISON OF ENTROPY AND GINI INDEX</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both entropy and gini index are used to compute which node should be taken as root node in the tree and which nodes should be taken in the subsequent levels.\n",
    "- But if we compare both the methods then gini impurity is more efficient than entropy in the terms of computing power.Computing Power indicates the processor time and memory.\n",
    "- The entropy values will be in the range of 0 to 1,wheares,gini values lies between 0 to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://www.researchgate.net/publication/339471092/figure/fig1/AS:862307349446657@1582601501720/Relation-among-Entropy-Gini-Index-and-Misclassification-error.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size: 24px\">PRE PRUNING</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pre pruning also known simply as \"Pruning\" in the context of decision trees is a technique used to control the size of a tree during its construction by stopping the tree building process before it becomes too complex.\n",
    "- Pre-Pruning Process :\n",
    "  * Tree Construction\n",
    "  * Evaluation\n",
    "  * Pruning decision\n",
    "  * Resulting pruned tree\n",
    "- Prepruning Tecninques:\n",
    "  * Maximum Depth\n",
    "  * Minimum Samples Split\n",
    "  * Minimum Samples leaf\n",
    "  * Maximum Features\n",
    "- Benefits:\n",
    "  * Prevents overfitting\n",
    "  * Computational Efficiency\n",
    "  * Improves Generalization    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size: 24px\">POST PRUNING</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Post pruning also known  as \"Pruning\" in the context of decision trees is a technique used to control the size of a fully grown tree after it has been constructed.\n",
    "- Post-Pruning Tecninque :\n",
    "  * Cost complexity\n",
    "  * Reduced error pruning\n",
    "  * Chi-square pruning \n",
    "-  Post-Pruning process :\n",
    "  * Tree Construction\n",
    "  * Evaluation\n",
    "  * Pruning decision\n",
    "  * Resulting pruned tree \n",
    "- Benefits:\n",
    "  * Fine tuning\n",
    "  * Reduces overfitting\n",
    "  * Adaptive   \n",
    "  * More accurate(growing complete tree)\n",
    "  * Remove unnecessary branch based on a given thresold.\n",
    "- Disadvantage :\n",
    "  * Computationally expensive\n",
    "  * Not use huge data   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:white;font-size: 24px\">ADVANTAGES</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision trees are easy to understand to interpret.\n",
    "- No need to Data Normalization.\n",
    "- Handle Non-linearity\n",
    "- Handling Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size: 24px\">DISADVANTAGES</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting\n",
    "- Global optimum\n",
    "- Sensitive to outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REF:\n",
    "- https://pradeep-dhote9.medium.com/decision-tree-classification-and-regression-algorithm-cart-id3-36a2450a7f1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>Heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender Heart_disease\n",
       "0      M           Yes\n",
       "1      F            No\n",
       "2      M           Yes\n",
       "3      F           Yes\n",
       "4      M            No"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'gender':['M','F','M','F','M'],\n",
    "    'Heart_disease':['Yes','No','Yes','Yes','No']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>Heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>180</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>225</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   weight Heart_disease\n",
       "0     220           Yes\n",
       "1     180           Yes\n",
       "2     225           Yes\n",
       "3     190            No\n",
       "4     155            No"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = {\n",
    "    'weight':[220,180,225,190,155],\n",
    "    'Heart_disease':['Yes','Yes','Yes','No','No']\n",
    "}\n",
    "df_2 = pd.DataFrame(data_2)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort your data (ascending order)\n",
    "- take avg of adjacent value\n",
    "- With respect to every avg value i have to do a split and either findout entropy or gini impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange;font-size: 24px\">DTC</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HyperParameters:\n",
    "  * Criterion(gini or entropy)\n",
    "  * max-deph\n",
    "  * min samples split\n",
    "  * min samples leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size: 24px\">DTR</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HyperParameters:\n",
    "  * Criterion(mse)\n",
    "  * max-deph\n",
    "  * min samples split\n",
    "  * min samples leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3rd scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>170</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>175</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height  Weight\n",
       "0     160      50\n",
       "1     165      65\n",
       "2     180      50\n",
       "3     170      85\n",
       "4     175      70"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'Height':[160,165,180,170,175],\n",
    "    'Weight':[50,65,50,85,70]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4th scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>F</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gender  Weight\n",
       "0      M      50\n",
       "1      F      65\n",
       "2      M      50\n",
       "3      F      85\n",
       "4      M      70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = {\n",
    "    'gender':['M','F','M','F','M'],\n",
    "    'Weight':[50,65,50,85,70]\n",
    "}\n",
    "df_2 = pd.DataFrame(data_2)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sort your data (ascending order)\n",
    "- take avg of adjacent value in independent column\n",
    "- Variance - Reduction in Variance(RIV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://scaler.com/topics/images/Reduction-in-variance.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/1*go8SVKCwq1Y5Ry1RHQC5XQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum RIV - Selected as Root Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Complex Data - Not use DT ,use Ensemble Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Classification :\n",
    "  * Gini impurity or Entropy - to select the root node\n",
    "- For Regression :\n",
    "  * Variance or Reduction of variance(RIV) - to select root node   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Purity -> check purity or impurity\n",
    "   * Entropy\n",
    "   * Gini Impurity\n",
    "2. What feature you need select for splitting\n",
    "   * Information Gain   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When dataset is small - use Entropy\n",
    "- When dataset is large - use Gini Impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
