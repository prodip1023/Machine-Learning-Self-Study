{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: yellow; font-size: 24px; font-weight: bold;\">\n",
    "    EXTREAM GRADIENT BOOSTING\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost is a powerful and widely used in Machine Learning library that implements gradient boosting algorithms.\n",
    "- It is designed for speed and performance and has become a popular choice for various Machine Learning tasks.\n",
    "- Including Classification and Regression and ranking(recommendation system) problems.\n",
    "- Max_depth = 6\n",
    "- Huge amount data :\n",
    "    * Spark (instead of pandas)\n",
    "    * Dask (instead of pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performance\n",
    "- Speed\n",
    "- Flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performance :\n",
    "  1. Handling Missing Values (Internally handle missing values)\n",
    "  2. Tree pruning\n",
    "  3. Regularized learning objective\n",
    "  4. Efficient aware split finding\n",
    "- Speed :\n",
    "  1. Parallel processing (CPU-GPU)\n",
    "  2. Distributed computing\n",
    "  3. Cache awareness \n",
    "  4. Optimized data structure\n",
    "- Flexibility :\n",
    "  1. Cross platform (Different types os + Hardware)\n",
    "  2. Multiple Language support\n",
    "  3. Integration with other library and tools\n",
    "  4. Support all kind of ML problems     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size: 24px\">STEP WISE EXTREAM GRADIENT BOOSTING INTITUTION</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For Classification\n",
    "1. Construct a base model.(probability of output target)\n",
    "2. Construct the decision tree with root.\n",
    "3. Calculate the similarity weights(score)\n",
    "\n",
    "similarity score(ss) = (sum of residuals)^2 /sum(1-p) + lamda (lamda means regularization parameter)\n",
    "\n",
    "4. calculate gain ( highest gain select feature)\n",
    "\n",
    "- For Regression\n",
    "1. Construct a base model.(average of the output or target)\n",
    "2. Construct the decision tree with root.\n",
    "3. Calculate the similarity weights(score)\n",
    "\n",
    "similarity score(ss) = (sum of residuals)^2 /num of residuals + lamda (lamda means regularization parameter)\n",
    "\n",
    "4. calculate gain ( highest gain select feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "1. https://arxiv.org/pdf/1603.02754\n",
    "2. https://github.com/dmlc/xgboost\n",
    "3. https://xgboost.readthedocs.io/en/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
