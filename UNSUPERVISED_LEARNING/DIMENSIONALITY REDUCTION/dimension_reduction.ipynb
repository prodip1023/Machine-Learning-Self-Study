{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNSUPERVISED DIMENSIONALITY REDUCTION VIA PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similar to feature selection,we can use different feature extraction techniques to reduce the number of features in a dataset.\n",
    "- The difference between feature selection and feature extraction is that while we maintain the original features when we use feature selection algorithms,such as sequential backward selection,we use feature extraction to transform or project the data onto a new feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Popular applications of PCA\n",
    "- EDA\n",
    "- Denoising signal in Stock Market trading\n",
    "- The Analysis of Genome Data and Gene Expression levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA helps us to identify patterns in data based on the correleation between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/05_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA ALGORITHM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Standardize the d-dimensional dataset\n",
    "2. Construct the covariance matrix\n",
    "3. Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "5. Select k eigenvectors,which correspond to the k largest eigenvalues,where k is the dimensionality of the new feature subspace(k<<d)\n",
    "6. Construct a projection matrix,W,from the \"top\" k eigenvectors.\n",
    "7. Transform the d-dimensional input dataset, X using the projection matrix,W,to obtain the new k-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUPERVISED DATA COMPRESSION VIA LINEAR DISCRIMINANT ANALYSIS(LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LDA can be used as a technique for feature extraction to increase computational efficiency and reduce the degree of overfitting due to the curse of dimensionality in non-regularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA and LDA are linear transformation technique can be used to reduce the number of dimensions in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/05_06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA ALGORITHM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Standardize the d-dimensional dataset(d is the number of features)\n",
    "2. For each class,compute the d-dimensional mean vector\n",
    "3. Construct the between-class scatter matrix,Sb,and the within-class scatter matrix,Sw\n",
    "4. Compute the eigenvectors and corresponding eigenvalues of the matrix ,Sw^-1.Sb\n",
    "5. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "6. Choose the k eigenvectors that correspond to the k largest eigenvalues to construct a d*k dimensional transformation matrix,W;the eigenvectors are the columns of this matrix.\n",
    "7. Project the examples onto the new feature subspace using the transformation matrix,W."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear transformation technique - PCA and LDA\n",
    "* Nonlinear dimensionality technique - t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NONLINEAR DIMENSIONALITY REDUCTION AND VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/05_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One nonlinear dimensionality reduction technique that is particularly worth highliting is t-distributed stochastic neighbor embedding(t-SNE) since it is frequently used in literature to visualize high-dimensional datasets in two or three dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](./Images/05_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
