{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; color: blue; font-size: 24px; font-weight: bold;\">\n",
    "    PRINCIPAL COMPONENT ANALYSIS(PCA)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA is a Machine Learning Technique or algorithm for dimensionality reduction.It means PCA is a tool to reduce multidimensional data to lower dimensions while retaining most of all data.\n",
    "- For example,we can use PCA to use reduce 7 dimensional data to a 3 dimensional data and still this 3 dimensinal data will have same contribution to the Machine Learning Model as the 7 dimensioanl data did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow;font-size: 24px\">ADVANTAGES</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The Speed of the Machine learning model wil improve.\n",
    "2. Data redundancy can be reduced.Data redundancy means duplicate the data.\n",
    "3. Visualization of data will become esay due to reduced number of dimensions.\n",
    "4. In case of images,audio and video files,PCA ignores small variations in the background.Only the variations around the peak are considered Hence noise in the images,audios and videos are reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING:\n",
    "Feature Engineering\n",
    "     \n",
    "      |\n",
    "\n",
    "Dimensionality Reduction(PCA) [Feature Extraction]\n",
    "\n",
    "      |\n",
    "\n",
    "Feature Selection  - Subset of feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURSE OF THE DIMENSIONALITY:\n",
    "\n",
    "f1 f2 f3 .......... f10 = (M1) - Accuracy = 75%\n",
    "\n",
    "f1 f2 f3 .......... f100 = (M2) - Accuracy = 80%\n",
    "\n",
    "f1 f2 f3 .......... f1000 = (M3) - Accuracy = 72% (overfitting or underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem:\n",
    "- When I am increase number of features my accuracy is also increasing,but when I am crossing the threshold or when we have large data or huge data or sparse data in that case accuracy is going to be decrease.\n",
    "- Curse of dimensinality\n",
    "##### Solution:\n",
    "- Dimension Reduction\n",
    "# Techniques:\n",
    "1. PCA (old technique)\n",
    "2. LDA (old technique)\n",
    "3. t-SNE(Distance embedding/similarity neighbourhood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red;font-size: 24px\">EIGEN VECTOR OR PRINCIPAL COMPONENT</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The dimensionality reduction concept entriely depend on creating the eigen vector that accomodates most of the data pionts properly.\n",
    "- Any quantity having direction as well as magnintude is called \"Vector\".\n",
    "- Eigen vector is created from Covarience Matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://miro.medium.com/v2/resize:fit:1400/0*yXt3gn1vmdY-LRcv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green;font-size: 24px\">COVARIANCE MATRIX</span>  ðŸš€."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Covariance represents the direction of relationship between two vatiables.\n",
    "- Positive nad Negative Covariance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When Covariance is calculated,the data should be first Standardized.(-1 to +1)\n",
    "- In the Covariance Matrix,the diagonal elements represent the variances (or derivations) of each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once Covariance Matrix is created,it contains the covariance values.From them,we have to pick up the Eigen values that represent magnitudes and Eigen Vectors that represent the directions.The Eigen value with highest magnitude should be selected and correseponding Eigen Vector should be selected.The Following is the total process:\n",
    "  1. PCA works on only normalized data.Hence we have to convert the given data into a maximum and minimum values.This can be done by MinMaxScaler or StandardScaler classes.\n",
    "  2. Create Covariance Matrix from the converted data.This can be done using cov() function of numpy package.\n",
    "  3. Calculate the Eigen Vectors and Eigen Values from the Covariance Matrix.This can be done using eig() function of 'linalg'(linear algebra) module in numpy package.\n",
    "  4. Extract only the Eigen value that is maximum and corresponding Eigen Vector.\n",
    "  5. Project the data on to that Eigen Vector.This is done by doing dot operation on the data with the Eigen Vector.For this purpose ,dot() method is used.\n",
    "  6. After the above step,the dimensionality of the data is reduced.This data with reduced dimensionality can be used in any Machine Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://daykemtainha.com/wp-content/uploads/2024/02/image-591.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "- Using PCA,we can reduce n dimensional data to lower dimensions.\n",
    "- Eigen vector * data = Pricipal Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigen Value and Eigen Vector:\n",
    "A * v = Lambda * v\\\n",
    "where,\\\n",
    "A = square matrix\\\n",
    "v =  vector\\\n",
    "Lambda = scaler value\n",
    "\n",
    "1. if we multiplying A (square matrix) with v(vector) and if we get a new vector lambda * v that is the lambda times of v(vector) ,then vector is called eigen vector of matrix A.\n",
    "2. if we multiply matrix A by vector v,the new vector lambda * v ,does not change the direction than only it is the eigen vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Eigen value tells us how much the eigen vector change in the size when we ,multiply with matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Transformation:\n",
    "https://shad.io/MatVis/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
